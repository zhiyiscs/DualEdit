edit_model_name: "minigpt-4-vicuna-7b"
llm_hidden_size: 4096
adaptor_mid_dim: 1024
adaptor_cross_att_head_n: 8
llm_layer_tmp: "llama_model.model.layers.{}"
llm_att_tmp: "llama_model.model.layers.{}.self_attn"
edit_layers: [17] 
train_cfg:
  lr: 1.e-4
  rel_lambda: 1
  gen_lambda: 1
  loc_lambda: 1
  inf_mapper_lambda: 0.1
IT:
  add_it: true
  layers: [18,19,20,21,22,23,24,25,26,27,28,29,30] 
  test_n: 1
  noise_level: 0.7 
  window: 0 
  vt_sample_n: 24
  mid_dim: 1024

  